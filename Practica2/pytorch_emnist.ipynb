{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikjGH5TSb1aY"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/028_pytorch_nn/pytorch_nn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjlQQ_qdb1ac"
      },
      "source": [
        "# Pytorch - Redes Neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O56G1hsMb1ad"
      },
      "source": [
        "En el post [anterior](https://sensioai.com/blog/027_pytorch_intro) hicimos una introducción al framework de `redes neuronales` `Pytorch`. Hablamos de sus tres elementos fundamentales: el objeto `tensor` (similar al `array` de `NumPy`) `autograd` (que nos permite calcular derivadas de manera automáticas) y el soporte GPU. En este post vamos a entrar en detalle en la  funcionalidad que nos ofrece la librería para diseñar redes neuronales de manera flexible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:21:45.471625Z",
          "start_time": "2020-08-15T12:21:45.002765Z"
        },
        "id": "t5yTdEiKb1ad"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Ce13k5MUb1af"
      },
      "source": [
        "## Modelos secuenciales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "5In16fdjb1af"
      },
      "source": [
        "La forma más sencilla de definir una `red neuronal` en `Pytorch` es utilizando la clase `Sequentail`. Esta clase nos permite definir una secuencia de capas, que se aplicarán de manera secuencial (las salidas de una capa serán la entrada de la siguiente). Ésto ya lo conocemos de posts anteriores, ya que es la forma ideal de definir un `Perceptrón Multicapa`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:21:45.486329Z",
          "start_time": "2020-08-15T12:21:45.472624Z"
        },
        "hidden": true,
        "id": "lTDpZ7bgb1af"
      },
      "outputs": [],
      "source": [
        "D_in, H1, H2, D_out = 784, 100, 50, 10\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H1, H2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H2, D_out),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "l979ABldb1ag"
      },
      "source": [
        "El modelo anterior es un `MLP` con 784 entradas, 100 neuronas en la capa oculta y 10 salidas. Podemos usar este modelo para hacer un clasificador de imágenes con el dataset MNIST. Pero primero, vamos a ver como podemos calcular las salidas del modelo a partir de unas entradas de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:21:45.502329Z",
          "start_time": "2020-08-15T12:21:45.487329Z"
        },
        "hidden": true,
        "id": "QjEQH-apb1ah",
        "outputId": "fcc7656a-ec8c-474e-ccf6-d5f5a6140c7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "outputs = model(torch.randn(64, 784))\n",
        "outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "tLaKHFEmb1ai"
      },
      "source": [
        "Como puedes ver, simplemente le pasamos los inputs al modelo (llamándolo como una función). En este caso, usamos un tensor con 64 vectores de 784 valores. Es importante remarcar que los modelos de `Pytorch` (por lo general) siempre esperan que la primera dimensión sea la dimensión *batch*. Si queremos entrenar esta red en una GPU, es tan sencillo como"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:21:46.777020Z",
          "start_time": "2020-08-15T12:21:45.503329Z"
        },
        "hidden": true,
        "id": "m-LI5mHLb1ai",
        "outputId": "dde7d848-1a4a-4f9a-912f-41c2ef2df2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MWmfc28cb1aj"
      },
      "source": [
        "Vamos a ver ahora como entrenar este modelo con el dataset MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:22:05.622262Z",
          "start_time": "2020-08-15T12:21:46.778019Z"
        },
        "hidden": true,
        "id": "DKuRv0klb1aj",
        "outputId": "722ccc49-5168-4c91-d6ae-d9ae094edafa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18799, 785)\n",
            "(18799,)\n",
            "(18799, 784)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/emnist.csv\")\n",
        "\n",
        "print(data.shape)\n",
        "y = data[data.columns[0]]\n",
        "x = data[data.columns[1:785]]\n",
        "y = np.array(y)\n",
        "x = np.array(x)\n",
        "print(y.shape)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:22:05.761911Z",
          "start_time": "2020-08-15T12:22:05.624102Z"
        },
        "hidden": true,
        "id": "GwJZNZQZb1aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6d6192-1dc1-45fc-af5d-0f09ef9fed33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-4a166dc07f2d>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train, X_test, y_train, y_test = X[:15000] / 255., X[15000:] / 255., Y[:15000].astype(np.int), Y[15000:].astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# normalización y split\n",
        "\n",
        "X_train, X_test, y_train, y_test = X[:15000] / 255., X[15000:] / 255., Y[:15000].astype(np.int), Y[15000:].astype(np.int)\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:22:05.777964Z",
          "start_time": "2020-08-15T12:22:05.763102Z"
        },
        "hidden": true,
        "id": "qf_5hi0Pb1ak"
      },
      "outputs": [],
      "source": [
        "# función de pérdida y derivada\n",
        "\n",
        "def softmax(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
        "\n",
        "def cross_entropy(output, target):\n",
        "    logits = output[torch.arange(len(output)), target]\n",
        "    loss = - logits + torch.log(torch.sum(torch.exp(output), axis=-1))\n",
        "    loss = loss.mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:22:07.268014Z",
          "start_time": "2020-08-15T12:22:05.778966Z"
        },
        "hidden": true,
        "id": "EcdY7J0ib1ak",
        "outputId": "976ac8d7-a3bc-480f-fa00-e310a9ad70c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/500 Loss 2.15910\n",
            "Epoch 20/500 Loss 2.00951\n",
            "Epoch 30/500 Loss 1.86740\n",
            "Epoch 40/500 Loss 1.65752\n",
            "Epoch 50/500 Loss 1.52898\n",
            "Epoch 60/500 Loss 1.39166\n",
            "Epoch 70/500 Loss 1.26216\n",
            "Epoch 80/500 Loss 1.15690\n",
            "Epoch 90/500 Loss 1.06202\n",
            "Epoch 100/500 Loss 0.99041\n",
            "Epoch 110/500 Loss 0.92373\n",
            "Epoch 120/500 Loss 0.86407\n",
            "Epoch 130/500 Loss 0.81342\n",
            "Epoch 140/500 Loss 0.78230\n",
            "Epoch 150/500 Loss 0.74795\n",
            "Epoch 160/500 Loss 0.71160\n",
            "Epoch 170/500 Loss 0.67866\n",
            "Epoch 180/500 Loss 0.64884\n",
            "Epoch 190/500 Loss 0.62465\n",
            "Epoch 200/500 Loss 0.60505\n",
            "Epoch 210/500 Loss 0.58210\n",
            "Epoch 220/500 Loss 0.56082\n",
            "Epoch 230/500 Loss 0.54109\n",
            "Epoch 240/500 Loss 0.52275\n",
            "Epoch 250/500 Loss 0.50569\n",
            "Epoch 260/500 Loss 0.48996\n",
            "Epoch 270/500 Loss 0.50354\n",
            "Epoch 280/500 Loss 0.51931\n",
            "Epoch 290/500 Loss 0.52334\n",
            "Epoch 300/500 Loss 0.52251\n",
            "Epoch 310/500 Loss 0.51184\n",
            "Epoch 320/500 Loss 0.50077\n",
            "Epoch 330/500 Loss 0.48983\n",
            "Epoch 340/500 Loss 0.49701\n",
            "Epoch 350/500 Loss 0.50194\n",
            "Epoch 360/500 Loss 0.50233\n",
            "Epoch 370/500 Loss 0.49542\n",
            "Epoch 380/500 Loss 0.48706\n",
            "Epoch 390/500 Loss 0.47852\n",
            "Epoch 400/500 Loss 0.47003\n",
            "Epoch 410/500 Loss 0.46169\n",
            "Epoch 420/500 Loss 0.45354\n",
            "Epoch 430/500 Loss 0.44561\n",
            "Epoch 440/500 Loss 0.43791\n",
            "Epoch 450/500 Loss 0.43047\n",
            "Epoch 460/500 Loss 0.42355\n",
            "Epoch 470/500 Loss 0.41962\n",
            "Epoch 480/500 Loss 0.41294\n",
            "Epoch 490/500 Loss 0.40634\n",
            "Epoch 500/500 Loss 0.39989\n"
          ]
        }
      ],
      "source": [
        "# convertimos datos a tensores y copiamos en gpu\n",
        "X_t = torch.from_numpy(X_train).float().cuda()\n",
        "Y_t = torch.from_numpy(y_train).long().cuda()\n",
        "\n",
        "# bucle entrenamiento\n",
        "epochs = 500\n",
        "lr = 0.9\n",
        "log_each = 10\n",
        "l = []\n",
        "for e in range(1, epochs+1): \n",
        "    \n",
        "    # forward\n",
        "    y_pred = model(X_t)\n",
        "\n",
        "    # loss\n",
        "    loss = cross_entropy(y_pred, Y_t)\n",
        "    l.append(loss.item())\n",
        "    \n",
        "    # ponemos a cero los gradientes\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backprop (calculamos todos los gradientes automáticamente)\n",
        "    loss.backward()\n",
        "\n",
        "    # update de los pesos\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= lr * param.grad\n",
        "    \n",
        "    if not e % log_each:\n",
        "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2QGLsxatb1al"
      },
      "source": [
        "Como puedes observar en el ejemplo, podemos calcular la salida del modelo con una simple línea. Luego calculamos la función de pérdida, y llamando a la función `backward` `Pytorch` se encarga de calcular las derivadas de la misma con respecto a todos los parámetros del modelo automáticamente (si no queremos acumular estos gradientes, nos aseguramos de llamar a la función `zero_grad` para ponerlos a cero antes de calcularlos). Por útlimo, podemos iterar por los parámetros del modelo aplicando la regla de actualización deseada (en este caso usamos `descenso por gradiente`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-15T12:22:07.312014Z",
          "start_time": "2020-08-15T12:22:07.270016Z"
        },
        "hidden": true,
        "id": "qZprtXOFb1al",
        "outputId": "f1b91fc1-76ea-4108-a311-1e77cd681beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9483272727272727"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate(x):\n",
        "    model.eval()\n",
        "    y_pred = model(x)\n",
        "    y_probas = softmax(y_pred)\n",
        "    return torch.argmax(y_probas, axis=1)\n",
        "\n",
        "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
        "accuracy_score(y_test, y_pred.cpu().numpy())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}